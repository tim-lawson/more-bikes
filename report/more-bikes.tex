\documentclass[11pt]{extarticle}

% Page layout
\usepackage{geometry}
\geometry{
  a4paper,
  margin={1in,1in},
}
\usepackage{setspace}

% Typography
% https://ctan.math.washington.edu/tex-archive/fonts/etbb/doc/ETbb-doc.pdf
\usepackage[p]{ETbb}
\usepackage[scaled=.95,type1]{cabin}
\usepackage[varqu,varl]{zi4}
\usepackage[T1]{fontenc}
\usepackage[libertine,vvarbb]{newtxmath}
\usepackage{microtype}
\frenchspacing

% Bibliography
\usepackage[
  style=authoryear-ibid,
  maxcitenames=2,
  doi=false,
  isbn=false,
  % Omit URL except for @online
  url=false,
]{biblatex}

\addbibresource{~/library.bib}

% Map @misc to @online
\DeclareSourcemap{
  \maps[datatype=bibtex, overwrite=true]{
    \map{
      \step[typesource=misc, typetarget=online]
    }
  }
}

\AtEveryBibitem{%
  \clearlist{language}%
  \clearfield{month}%
  \clearfield{note}%
  \clearfield{eprint}%
  % Omit "Visited on <date>"
  \iffieldundef{urlyear}
    {}
    {\clearfield{urlyear}}
}

% Quotations
\usepackage{csquotes}

% Input/include relative paths
\usepackage{import}

% References
\usepackage{hyperref}
\usepackage{cleveref}

% Math
\usepackage{amsmath}
\usepackage{mathtools}

% Tables and charts
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{array}
\usepackage{makecell}
\usepackage{multirow}
\usepackage[draft]{graphicx}

\def\imagebox#1#2{\vtop to #1{\null\hbox{#2}\vfill}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash }b{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash }b{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash }b{#1}}

\usepackage{tikz}
\usetikzlibrary{external}
\tikzexternalize[prefix=figures/]

\usepackage{pgfplots}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.17}
\usepgfplotslibrary{colorbrewer}
\usepgfplotslibrary{fillbetween}
\usepgfplotslibrary{groupplots}

% Exclusive filter
\pgfplotsset{
  discard if/.style 2 args={
      x filter/.append code={
          \readlist\mylist{#2}%
          \foreachitem\z\in\mylist[]{%
            \ifdim\thisrow{#1} pt=\z pt
              \def\pgfmathresult{inf}
            \fi
          }
        }
    },
}

% Inclusive filter
\pgfplotsset{
  discard if not/.style 2 args={
      x filter/.code={
          \edef\tempa{\thisrow{#1}}
          \edef\tempb{#2}
          \ifx\tempa\tempb
          \else
            \def\pgfmathresult{inf}
          \fi
        }
    }
}

% Comments
\setlength{\marginparwidth}{1in}
\usepackage{todonotes}
\newcounter{todocounter}
\newcommand{\todonum}[1]{%
  \stepcounter{todocounter}%
  \todo[color={red!100!green!33},inline,size=\small]{
    \thetodocounter: #1
  }%
}

\usepackage{siunitx}

\usepackage{listings}
\lstset{
    basicstyle=\tt,
}
\newcommand{\sklearn}[1]{
  \href{https://scikit-learn.org/stable/modules/generated/sklearn.#1.html}{\lstinline|sklearn.#1|}
}

\newcommand{\isholiday}{is\_holiday}
\newcommand{\windspeedmax}{wind\_speed\_max}
\newcommand{\windspeedavg}{wind\_speed\_avg}
\newcommand{\winddirection}{wind\_direction}
\newcommand{\bikesavgfull}{bikes\_avg\_full}
\newcommand{\bikesavgshort}{bikes\_avg\_short}
\newcommand{\bikesh}{bikes\_3h}
\newcommand{\bikeshdiffavgfull}{bikes\_3h\_diff\_avg\_full}
\newcommand{\bikeshdiffavgshort}{bikes\_3h\_diff\_avg\_short}

\begin{document}

\title{More Bikes: Experiments in Univariate Regression}
\author{Tim Lawson}
\date{\today}

\maketitle

\section{Task description}

The assignment is to predict the number of available bikes at 75 rental stations in
three hours' time for a period of three months beginning in November 2014, i.e., a
supervised univariate regression problem.
It is divided into three sub-tasks, which differ in the information that is available:
\begin{enumerate}
  \item The number of available bikes at each of the 75 stations for the month of October 2014.
        This sub-task may be approached by building a separate model for each station or a
        single model for all 75 stations.
  \item A set of linear models that were trained on the number of available bikes at each of a
        separate set of 200 stations for a year.
  \item Both of the above.
\end{enumerate}
Additionally, the number of available bikes at each of the first ten stations is
available for the period from June 2012 to October 2014 for analysis.
The predictions are evaluated by the mean absolute error (MAE) between the predicted
and true numbers of available bikes over the period of three months, beginning in
November 2014.
The evaluation data was not available to participants; however, the score achieved on a
held-out test set is reported on the task
leaderboard\footnote{\href{https://www.kaggle.com/competitions/morebikes2023/leaderboard}{\texttt{https://www.kaggle.com/competitions/morebikes2023/leaderboard}}}.

This report begins with a description of the general approach taken to the assignment in \cref{sec:approach}.
Then, \cref{sec:data-analysis} presents a preliminary analysis of the data.
Finally, \cref{sec:results} details the results of the methods applied to each of the sub-tasks.

\section{Approach}
\label{sec:approach}

The experiments described in this report were performed with the \texttt{scikit-learn}
Python package \parencite{Pedregosa2011}.
In each case, preprocessing and feature selection were performed by `estimators' that
implemented the `transformer' interface; prediction was performed by estimators that
implemented the `predictor' interface; and estimators were composed into
\texttt{Pipeline} objects over which hyperparameter search could be performed
\parencite[4-9]{Buitinck2013}.

Generally, standard $k$-fold cross-validation is disfavoured for time-series data due
to the inherent correlation between successive folds \parencite{Bergmeir2018}.
Instead, nested time-series
cross-validation\footnote{See \sklearn{model\_selection.TimeSeriesSplit}.} with ten folds
was used to evaluate the models.
This behaviour is illustrated in \cref{fig:chart-cross-validation}.
Hyperparameter tuning was performed by grid
search\footnote{See \sklearn{model\_selection.GridSearchCV}.} with the mean absolute
error as the scoring function, following the task description.
Finally, the statistical significance of the differences in performance between models
was assessed by paired $t$-tests of the scores achieved on the cross-validation
folds.

\import{.}{figure-cross-validation.tex}

\section{Data analysis}
\label{sec:data-analysis}

\import{.}{table-features.tex}

The data is at hourly intervals with $n = 54385$ instances across 75 stations.
A summary of its features and the distributional characteristics of the non-temporal
quantitative features are given in \cref{tab:features,tab:feature-stats} respectively.
This shows that many of the features, including the target variable
\texttt{bikes}, are missing for $n = 73$ instances, which were excluded from the analysis.
Additionally, the `profile' features, i.e., the features derived from the numbers of
available bikes at preceding times, are not defined for the first week of instances at
each station.
Hence, they are missing for approximately $\frac{1}{4}$ of the instances.
The meteorological features are constant for all stations at a given timestamp.
Naturally, the number of available bikes at a given station is bounded by zero and the
number of docks at that station.

\import{.}{table-feature-stats.tex}

\subsection{Feature selection}
\label{sec:feature-selection}

Intuitively, features that have zero variance are not informative for regression
analysis and were automatically
excluded\footnote{See \sklearn{feature\_selection.VarianceThreshold}.}.
In the case of sub-task~1, the available data is limited to the month of October 2014; therefore, these included the month and year.
Additionally, the `station' features (\cref{tab:features}) are constant for all
instances at a given station; hence, for the first case of sub-task~1, these were
excluded.
Finally, the \texttt{precipitation} feature is zero for all instances.

Correlations between features and, more generally, multicollinearity, are undesirable in regression analysis \parencite{Alin2010}.
Therefore, I sought to identify and exclude redundant features.
To determine these, I computed the Pearson correlation coefficients between pairs of quantitative features (\cref{fig:chart-correlations}).
This analysis yielded the following observations:
\begin{itemize}
  \item \texttt{\bikesavgfull} and \texttt{\bikesavgshort} are fully correlated ($r = 1.00$);
  \item \texttt{\bikeshdiffavgfull} and \texttt{\bikeshdiffavgshort} are fully correlated ($r  = 1.00$); and
  \item \texttt{\windspeedmax} and \texttt{\windspeedavg} are highly correlated ($r = 0.96$).
\end{itemize}
Hence, the second of each of these pairs of features was also excluded.

\import{.}{figure-correlations.tex}
% \import{.}{figure-weekday-separate.tex}
% \import{.}{figure-distributions.tex}

\subsection{Feature engineering}
\label{sec:feature-engineering}

The bounds on the number of available bikes at a given station were enforced by
predicting the \emph{fraction} of bikes, i.e., the number of bikes divided by the number
of docks at the station.
In each case, this was implemented by extending the \texttt{TransformedTargetRegressor}
meta-estimator to permit data-dependent
transforms\footnote{See \sklearn{compose.TransformedTargetRegressor}.}.
I note that, while tree models are invariant to monotonic transformations of explanatory
features \parencite[e.g.,][3184]{Death2000}, this does not apply to a data-dependent
transformation of the target variable.

\import{.}{figure-weekday.tex}

Initially, I considered the possibility of deriving temporal features, e.g.,
discretising the hour of the day into `day' and `night' intervals.
However, as shown in \cref{fig:chart-weekday}, the average fraction of available bikes
at each hour of the day differs substantially between the training data (top)
and the data from the first ten stations (bottom).
Additionally, having elected to investigate tree models for sub-task 1
(\cref{sec:results-subtask-1}), discretisation is less appropriate because a tree
naturally partitions the spaces of quantitative features \parencite[155]{Flach2012}.

\section{Results}
\label{sec:results}

\subsection{Sub-task 1}
\label{sec:results-subtask-1}

Gradient-boosted decision trees are a popular choice for time-series forecasting
problems
\parencite{Bojer2021}.
In particular, the \texttt{HistGradientBoostingRegressor}
estimator\footnote{See \sklearn{ensemble.HistGradientBoostingRegressor}.}, inspired by
the \texttt{LightGBM} implementation of gradient-boosted decision trees
\parencite{Ke2017}, supports missing values, which are evident in the data
(\cref{tab:feature-stats}).
Hence, for sub-task 1, I elected to focus on this class of models and to compare their
performance to individual decision trees.
As a baseline, I predicted the arithmetic mean fraction of available bikes at each station.

\begin{figure}
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{lrr}
      \toprule
      Model                                  & $\mu$ & $\sigma^2$
      \\
      \midrule
      Mean                                   & 4.43  & 3.82
      \\
      \texttt{DecisionTreeRegressor}         & 3.49  & 2.57
      \\
      \texttt{HistGradientBoostingRegressor} & 3.15  & 2.40
      \\
      \bottomrule
    \end{tabular}
    \caption{Separate model for each station}
    \label{fig:chart-subtask-1-1}
  \end{subfigure}
  \par\bigskip\bigskip
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{lrr}
      \toprule
      Model                                  & $\mu$ & $\sigma^2$
      \\
      \midrule
      Mean                                   & 5.45  & 0.06
      \\
      \texttt{HistGradientBoostingRegressor} & 2.60  & 0.08
      \\
      \texttt{LGBMRegressor}                 & 2.68  & 0.14
      \\
      \texttt{MLPRegressor}                  & 2.81  & 0.21
      \\
      \bottomrule
    \end{tabular}
    \caption{Single model for all stations}
    \label{fig:chart-subtask-1-2}
  \end{subfigure}
  \caption{The mean average error achieved by the classes of model for the cases of sub-task~1.
    In \cref{fig:chart-subtask-1-1}, $\mu$ is the average over stations and cross-validation folds.
    In \cref{fig:chart-subtask-1-2}, $\mu$ is the average over cross-validation folds only.
  }
  \label{fig:chart-subtask-1}
\end{figure}

\printbibliography

\end{document}
