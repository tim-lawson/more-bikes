\documentclass[11pt]{extarticle}
\usepackage{more-bikes}

% Shortcuts
\newcommand{\isholiday}{\texttt{is\_holiday}}
\newcommand{\windspeedmax}{\texttt{wind\_speed\_max}}
\newcommand{\windspeedavg}{\texttt{wind\_speed\_avg}}
\newcommand{\winddirection}{\texttt{wind\_direction}}
\newcommand{\bikesavgfull}{\texttt{bikes\_avg\_full}}
\newcommand{\bikesavgshort}{\texttt{bikes\_avg\_short}}
\newcommand{\bikesh}{\texttt{bikes\_3h}}
\newcommand{\bikeshdiffavgfull}{\texttt{bikes\_3h\_diff\_avg\_full}}
\newcommand{\bikeshdiffavgshort}{\texttt{bikes\_3h\_diff\_avg\_short}}

\newcommand{\rlmfull}{\texttt{rlm\_full}}
\newcommand{\rlmfulltemp}{\texttt{rlm\_full\_temp}}
\newcommand{\rlmshort}{\texttt{rlm\_short}}
\newcommand{\rlmshortfull}{\texttt{rlm\_short\_full}}
\newcommand{\rlmshortfulltemp}{\texttt{rlm\_short\_full\_temp}}
\newcommand{\rlmshorttemp}{\texttt{rlm\_short\_temp}}

\newcommand{\None}{\texttt{None}}

\newcommand{\kaggle}{https://www.kaggle.com/c/morebikes2023/leaderboard}
\newcommand{\sklearn}[2]{\href{https://scikit-learn.org/stable/modules/generated/sklearn.#1.#2.html}{\lstinline|sklearn.#1.#2|}}

\begin{document}

\title{`More Bikes': Experiments in Univariate Regression}
\author{Tim Lawson}
\date{\today}

\maketitle

\section{Task description}

The assignment is to predict the number of available bikes at 75 rental stations in
three hours' time for a period of three months from November 2014, i.e., a supervised
univariate regression problem.
It is divided into three sub-tasks, which differ in the information that is available:
\begin{enumerate}
  \item The number of available bikes at each of the 75 stations for the month of October 2014.
        This sub-task may be approached by building a separate model for each station or a
        single model for all 75 stations.
  \item A set of linear models that were trained on the number of available bikes at each of a
        separate set of 200 stations for a year.
  \item Both of the above.
\end{enumerate}
Additionally, the number of available bikes at each of ten stations between June 2012
and October 2014 is provided for analysis (\cref{fig:chart-weekday}).
The predictions are evaluated by the mean absolute error (MAE) between the predicted
and true numbers of available bikes over the period of three months, beginning in
November 2014.
The evaluation data was not available to participants; however, the score achieved on a
held-out test set is reported on the task
leaderboard.\footnote{\href{\kaggle}{\texttt{\kaggle}}} This report begins with a
description of the general approach taken to the assignment in \cref{sec:approach};
\cref{sec:data-analysis} presents a preliminary analysis of the data.
Finally, \cref{sec:results-subtask-1,sec:results-subtask-2} detail the results of the
methods applied to each of the sub-tasks.

\section{Approach}
\label{sec:approach}

The experiments described in this report were performed with the \texttt{scikit-learn}
Python package \parencite{Pedregosa2011}.
In each case, preprocessing and feature selection were performed by `estimators' that
implemented the `transformer' interface; prediction was performed by estimators that
implemented the `predictor' interface; and estimators were composed into
\texttt{Pipeline} objects over which hyperparameter search could be performed
\parencite[4-9]{Buitinck2013}.

Generally, standard $k$-fold cross-validation is disfavoured for time-series data due
to the inherent correlation between successive folds \parencite{Bergmeir2018}.
Instead, nested time-series cross-validation\footnote{See
  \sklearn{model\_selection}{TimeSeriesSplit}.
} with ten folds
was used to evaluate the models.
This behaviour is illustrated in \cref{fig:chart-cross-validation}.
Hyperparameter tuning was performed by grid search\footnote{See
  \sklearn{model\_selection}{GridSearchCV} and
  \sklearn{model\_selection}{HalvingGridSearchCV}.
}
with the mean absolute error as the scoring function, following the task description.
Finally, the statistical significance of the differences in performance between models
was assessed by paired $t$-tests of the scores achieved on the cross-validation folds.
These methods are described in more detail in
\cref{sec:results-subtask-1,sec:results-subtask-2}.

\import{}{figure-cross-validation.tex}

\section{Data analysis}
\label{sec:data-analysis}

\import{}{table-features.tex}

The data is at hourly intervals with $n = 54385$ instances across 75 stations.
A summary of its features and the distributional characteristics of the non-temporal
quantitative features are given in \cref{tab:features,tab:feature-stats} respectively.
This shows that many of the features, including the target variable \texttt{bikes}, are
missing for $n = 73$ instances, which were excluded from the analysis.
Additionally, the `profile' features, i.e., the features derived from the numbers of
available bikes at preceding times, are not defined for the first week of instances at
each station.
Hence, they are missing for approximately $\frac{1}{4}$ of the instances.
The meteorological features are constant for all stations at a given timestamp.
Finally, the number of available bikes at a given station is bounded by zero and the
number of docks at that station.

\subsection{Feature selection}
\label{sec:feature-selection}

\import{}{table-feature-stats.tex}

Intuitively, features that have zero variance are not informative for regression
analysis and were automatically excluded.\footnote{See
  \sklearn{feature\_selection}{VarianceThreshold}.
}
In the case of sub-task~1, the available data is limited to the month of October 2014;
therefore, these included the month and year.
Additionally, the `station' features (\cref{tab:features}) are constant for all
instances at a given station; hence, for the first case of sub-task~1, these were
excluded.
Finally, the \texttt{precipitation} feature is zero for all instances.

Correlations between features are undesirable in regression analysis
\parencite{Alin2010}.
Therefore, I sought to identify and exclude redundant features.
To determine these, I computed the Pearson correlation coefficients between pairs of
quantitative features (\cref{fig:chart-correlations}).
This analysis yielded the following observations:
\begin{itemize}
  \item \texttt{\bikesavgfull} and \texttt{\bikesavgshort} are fully correlated ($r = 1.00$);
  \item \texttt{\bikeshdiffavgfull} and \texttt{\bikeshdiffavgshort} are fully correlated ($r  = 1.00$);
  \item \texttt{\windspeedmax} and \texttt{\windspeedavg} are highly correlated ($r = 0.96$).
\end{itemize}
Hence, the second of each of these pairs of features was also excluded for sub-task~1.
For sub-task~2, the `profile' features were retained because they are required by the
pre-trained linear models.

\import{}{figure-correlations.tex}

\subsection{Feature engineering}
\label{sec:feature-engineering}

The bounds on the number of available bikes at a given station were enforced for
sub-task~1 by predicting the \emph{fraction} of bikes, i.e., the number of bikes
divided by the number of docks at the station.
I implemented this by extending the \texttt{TransformedTargetRegressor} meta-estimator
to permit data-dependent transforms.\footnote{See
  \sklearn{compose}{TransformedTargetRegressor}.
}
Another possible transformation of the target variable would have been to predict the
\emph{change} in the number of available bikes, but I did not explore this option.
I note that, while tree models are invariant to monotonic transformations of
explanatory features \parencite[e.g.,][3184]{Death2000}, this does not apply to a
data-dependent transformation of the target variable.

\import{}{figure-weekday.tex}

Initially, I investigated introducing derived temporal features, e.g., by discretizing
the hour of the day into `day' and `night' intervals.
However, as shown in \cref{fig:chart-weekday}, the average fraction of available bikes
at each hour of the day differs substantially between the training data and the data
from the first ten stations.
Hence, I considered that a feature of this kind would be unlikely to generalize well to
the test data.
Additionally, having elected to investigate tree models for sub-task 1
(\cref{sec:results-subtask-1}), which partition the spaces of quantitative features
\parencite[155]{Flach2012}, discretization may be unnecessary.

\section{Sub-task 1: Decision trees}
\label{sec:results-subtask-1}

Gradient-boosted decision trees (GBDTs) are a popular choice of model class for
time-series forecasting problems \parencite{Bojer2021}.
For both cases of sub-task 1, I chose to investigate the performance of decision trees
and ensemble methods based on decision trees.
In particular, \texttt{scikit-learn} provides an optimized implementation\footnote{See
  \sklearn{ensemble}{HistGradientBoostingRegressor}.
} of GBDTs,
inspired by LightGBM \parencite{Ke2017}, that supports missing values (cf. \cref{tab:feature-stats}).
I undertook to compare its performance to that of a single decision tree.
Additionally, I predicted the arithmetic mean of the fraction of available bikes as a
baseline.

The best estimator for each model class was determined by grid search with ten-fold
cross-validation (\cref{sec:approach}).
However, with a separate model for each of 75 stations, I found that it was
computationally expensive to perform grid search over a wide range of hyperparameters.
Hence, I opted to constrain the search space.
This was achieved by performing grid search for the case of a single model for all
stations, then varying the parameters about the values of the best estimator.
The resultant parameter grids for decision trees and gradient-boosted decision trees
are given in \cref{tab:chart-parameters-subtask-1}.

\begin{table}
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{ll}
      \toprule
      Parameter                   & Values
      \\
      \midrule
      \texttt{criterion}          & \texttt{absolute\_error}
      \\
      \midrule
      \texttt{max\_depth}         & \None, 10, 20, 50
      \\
      \texttt{min\_samples\_leaf} & 5, 10, 20
      \\
      \texttt{max\_leaf\_nodes}   & \None, 7, 15, 31
      \\
      \bottomrule
    \end{tabular}
    \caption{Decision tree}
    \label{tab:chart-parameters-subtask-1-1}
  \end{subfigure}
  \par\bigskip\bigskip
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{ll}
      \toprule
      Parameter                   & Values
      \\
      \midrule
      \texttt{loss}               & \texttt{absolute\_error}
      \\
      \texttt{scoring}            & \texttt{neg\_mean\_absolute\_error}
      \\
      \midrule
      \texttt{l2\_regularization} & 0.1, 0.2, \underline{0.5}
      \\
      \texttt{max\_depth}         & \None, 5, 10, \underline{20}, 50
      \\
      \texttt{max\_iter}          & 10, 20, 50, 100, \underline{200}, 500
      \\
      \texttt{max\_leaf\_nodes}   & \None, \underline{15}, 31, 63
      \\
      \texttt{min\_samples\_leaf} & 1, 2, 5, \underline{10}, 20, 50
      \\
      \bottomrule
    \end{tabular}
    \caption{Gradient-boosted decision tree}
    \label{tab:chart-parameters-subtask-1-2}
  \end{subfigure}
  \caption{The parameter grids over which hyperparameter search was performed for sub-task~1.
    Except where stated, the default values of the parameters were used.
    Scoring criteria were fixed to the mean absolute error, following the task description.
    The best parameters for the case of a single model for all stations are underlined.
  }
  \label{tab:chart-parameters-subtask-1}
\end{table}

The mean average errors achieved by the best estimator for each model class over the
cross-validation folds and stations are given in \cref{tab:chart-subtask-1}.
With a separate model for each station, there is greater variance in the mean average
error because there were fewer instances per cross-validation fold.
Intuitively, the baseline achieved a lower mean average error with a separate model for
each station.
In both cases, all model classes achieved a lower mean average error than the baseline.

\begin{table}
  \centering
  \begin{subfigure}{0.49\textwidth}
    \centering
    \begin{tabular}{lrr}
      \toprule
      Model                          & $\mu$ & $\sigma^2$
      \\
      \midrule
      Baseline                       & 4.43  & 3.82
      \\
      Decision tree                  & 3.38  & 3.05
      \\
      Gradient-boosted decision tree & 3.23  & 2.54
      \\
      \bottomrule
    \end{tabular}
    \caption{Separate model for each station}
    \label{tab:chart-subtask-1-1}
  \end{subfigure}
  % \par\bigskip\bigskip
  \begin{subfigure}{0.49\textwidth}
    \centering
    \begin{tabular}{lrr}
      \toprule
      Model                          & $\mu$ & $\sigma^2$
      \\
      \midrule
      Baseline                       & 5.45  & 0.06
      \\
      Decision tree                  & 3.11  & 0.05
      \\
      Gradient-boosted decision tree & 2.60  & 0.08
      \\
      % \texttt{LGBMRegressor}                 & 2.76  & 0.18
      % \\
      \bottomrule
    \end{tabular}
    \caption{Single model for all stations}
    \label{tab:chart-subtask-1-2}
  \end{subfigure}
  \caption{The mean average error achieved by the classes of model for the cases of sub-task~1.
    In \cref{tab:chart-subtask-1-1}, $\mu$ is the average over stations and
    cross-validation folds.
    In \cref{tab:chart-subtask-1-2}, $\mu$ is the average over cross-validation folds only.
  }
  \label{tab:chart-subtask-1}
\end{table}

Paired $t$-tests of the scores achieved by the best estimators for each model class
were used to determine whether the mean average error over cross-validation folds was
significantly different.
I did not compare the scores between the two cases of sub-task~1 because the samples
are different.
With a separate model for each station, both individual decision trees and GBDTs
performed significantly better than the baseline for roughly $\frac{2}{3}$ of the
stations (\cref{tab:chart-subtask-1-t-tests-1}).
GBDTs performed significantly better than individual decision trees for $8$ stations
only.
With a single model for all stations, the results were more decisive -- a GBDT
performed significantly better than a decision tree
(\cref{tab:chart-subtask-1-t-tests-2}).

\begin{table}
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{llrrr}
      \toprule
      Model 1       & Model 2                        & $\mu_t$ & $t > 0$ & $p < 0.05$
      \\
      \midrule
      Baseline      & Decision tree                  & 2.957   & 71      & 47
      \\
      Baseline      & Gradient-boosted decision tree & 3.399   & 73      & 49
      \\
      Decision tree & Gradient-boosted decision tree & 0.452   & 47      & 8
      \\
      \bottomrule
    \end{tabular}
    \caption{
      Separate models for each station.
      The average $t$-statistic ($\mu_t$), number of positive $t$-statistics ($t > 0$), and
      number of significant $p$-values ($< 0.05$) for paired $t$-tests of the scores achieved
      by the best estimators for each model class.
    }
    \label{tab:chart-subtask-1-t-tests-1}
  \end{subfigure}
  \par\bigskip\bigskip
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{llrr}
      \toprule
      Model 1       & Model 2                        & $t$-statistic & $p$-value
      \\
      \midrule
      Baseline      & Decision tree                  & 25.2          & \num{1.16e-9}
      \\
      Baseline      & Gradient-boosted decision tree & 29.4          & \num{2.99e-10}
      \\
      % Baseline      & LightGBM                       & 23.8 & \num{1.97e-9}
      % \\
      % Decision tree & LightGBM                       & 4.0  & \num{3.23e-3}
      % \\
      % LightGBM      & Gradient-boosted decision tree & 3.1  & \num{1.30e-2}
      % \\
      Decision tree & Gradient-boosted decision tree & 13.2          & \num{3.43e-7}
      \\
      \bottomrule
    \end{tabular}
    \caption{ Single model for all stations.
      The test statistic $t$ and $p$-value for paired $t$-tests of the scores achieved by the
      best estimators for each model class.
    }
    \label{tab:chart-subtask-1-t-tests-2}
  \end{subfigure}
  \caption{The results of paired $t$-tests of the scores achieved by the best
    estimators for each model class for the cases of sub-task~1.
  }
\end{table}

\section{Sub-task 2: Ensembles}
\label{sec:results-subtask-2}

The available information for sub-task 2 is a set of linear models that were trained on
the data for a separate set of 200 stations for a year.
For each station, there are six models that were generated by the \texttt{rlm} function
from the R package MASS.\footnote{See
  \href{https://www.rdocumentation.org/packages/MASS/versions/7.3-54/topics/rlm}{\texttt{https://www.rdocumentation.org/packages/MASS/versions/7.3-54/topics/rlm}}.
}
Each of the models uses a different set of features, which are listed in
\cref{tab:features-subtask-2}.
As discussed in \cref{sec:feature-selection}, the `full' and `short' features are fully
correlated.

\begin{table}
  \centering
  \begin{tabular}{lllllll}
    Model                      & \rot{\bikesh{}}             & \rot{\bikesavgfull{}}      & \rot{\bikesavgshort{}} &
    \rot{\bikeshdiffavgfull{}} & \rot{\bikeshdiffavgshort{}} & \rot{\texttt{temperature}}
    \\
    \midrule \rlmfull          & \checkmark                  & \checkmark                 &                        & \checkmark &            &
    \\
    \rlmfulltemp               &
    \checkmark                 & \checkmark                  &                            & \checkmark             &            & \checkmark
    \\
    \rlmshort                  & \checkmark                  &                            &
    \checkmark                 &                             & \checkmark                 &
    \\
    \rlmshortfull              & \checkmark                  & \checkmark                 & \checkmark             &
    \checkmark                 & \checkmark                  &
    \\
    \rlmshortfulltemp          & \checkmark                  & \checkmark                 & \checkmark             &
    \checkmark                 & \checkmark                  & \checkmark
    \\
    \rlmshorttemp              & \checkmark                  &                            & \checkmark             &            &
    \checkmark                 & \checkmark
    \\
    \bottomrule
  \end{tabular}
  \caption{The features used by the
    linear models for sub-task~2.
    The features are defined in \cref{tab:features}.
  }
  \label{tab:features-subtask-2}
\end{table}

\printbibliography

\end{document}
