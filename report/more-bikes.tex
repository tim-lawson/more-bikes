\documentclass[11pt]{extarticle}
\usepackage{more-bikes}

% Shortcuts
\newcommand{\isholiday}{\texttt{is\_holiday}}
\newcommand{\windspeedmax}{\texttt{wind\_speed\_max}}
\newcommand{\windspeedavg}{\texttt{wind\_speed\_avg}}
\newcommand{\winddirection}{\texttt{wind\_direction}}
\newcommand{\bikesavgfull}{\texttt{bikes\_avg\_full}}
\newcommand{\bikesavgshort}{\texttt{bikes\_avg\_short}}
\newcommand{\bikesh}{\texttt{bikes\_3h}}
\newcommand{\bikeshdiffavgfull}{\texttt{bikes\_3h\_diff\_avg\_full}}
\newcommand{\bikeshdiffavgshort}{\texttt{bikes\_3h\_diff\_avg\_short}}

\newcommand{\rlmfull}{\texttt{rlm\_full}}
\newcommand{\rlmfulltemp}{\texttt{rlm\_full\_temp}}
\newcommand{\rlmshort}{\texttt{rlm\_short}}
\newcommand{\rlmshortfull}{\texttt{rlm\_short\_full}}
\newcommand{\rlmshortfulltemp}{\texttt{rlm\_short\_full\_temp}}
\newcommand{\rlmshorttemp}{\texttt{rlm\_short\_temp}}

\newcommand{\None}{\texttt{None}}

\newcommand{\kaggle}{https://www.kaggle.com/c/morebikes2023/leaderboard}
\newcommand{\sklearn}[2]{\href{https://scikit-learn.org/stable/modules/generated/sklearn.#1.#2.html}{\lstinline|sklearn.#1.#2|}}

\begin{document}

\title{`More Bikes': Experiments in Univariate Regression}
\author[]{Tim Lawson}
\affil[]{University of Bristol}
\affil[]{\texttt{tim.lawson@bristol.ac.uk}}
\date{}

\maketitle

\section{Task description}
\label{sec:task-description}

The assignment is to predict the number of available bikes at 75 rental stations in
three hours' time for a period of three months from November 2014, i.e., a supervised
univariate regression problem.
It is divided into three sub-tasks, which differ in the information that is available.
For sub-task~1, the number of available bikes at each of the 75 stations for the month
of October 2014 is provided (\cref{sec:subtask-1}).
This sub-task may be approached by building a separate model for each station or a
single model for all 75 stations.
For sub-task~2, a set of linear models that were trained on the number of available
bikes at each of a separate set of 200 stations are provided (\cref{sec:subtask-2}).
Finally, for sub-task~3, both sources of information may be used.
Additionally, the number of available bikes at the first ten stations between June 2012
and October 2014 is provided for analysis (see \cref{fig:weekday}).

The predictions are evaluated by the mean absolute error (MAE) between the predicted
and true numbers of available bikes.
Hereafter, the MAE is referred to as the `score'.
\begin{equation}
  \label{eq:mae}
  \text{MAE} = \frac{1}{n} \sum_{i = 1}^n \lvert y_i - \hat{y}_i \rvert
\end{equation}
The evaluation data was not available to participants, but the score achieved on a
held-out test set is reported on the task
leaderboard.\footnote{\href{\kaggle}{\texttt{\kaggle}}} I give the score achieved on
this test set by the best estimators for each model class and sub-task alongside the
mean scores achieved on cross-validation folds of the data provided for sub-task~1 in
TODO.

This report begins with a description of the general approach taken to the assignment
in \cref{sec:approach}.
\Cref{sec:data-analysis} presents a preliminary analysis of the data.
Finally, \cref{sec:subtask-1,sec:subtask-2} detail the results of the methods applied
to each of the sub-tasks.\footnote{The code that produced these results is available at
  \github{https://github.com/tslwn/more-bikes}{tslwn/more-bikes}.
}

\section{Approach}
\label{sec:approach}

I used the \texttt{scikit-learn} Python package \parencite{Pedregosa2011} throughout
this report.
In each case, preprocessing and feature selection were performed by \emph{estimators}
that implemented the \emph{transformer} interface, prediction was performed by
estimators that implemented the \emph{predictor} interface, and estimators were
composed into \texttt{Pipeline} objects over which hyperparameter search was performed
\parencite[4-9]{Buitinck2013}.

Generally, standard $k$-fold cross-validation is disfavoured for time-series data due
to the inherent correlation between successive folds \parencite{Bergmeir2018}.
Instead, I used nested time-series cross-validation\footnote{See
  \sklearn{model\_selection}{TimeSeriesSplit}.
} with ten folds to evaluate the models, which is illustrated in
\cref{fig:cross-validation}.
I determined the best estimator for each model class by grid search\footnote{See
  \sklearn{model\_selection}{GridSearchCV} and
  \sklearn{model\_selection}{HalvingGridSearchCV}.
} with the mean absolute
error as the scoring function, following the task description (\cref{sec:task-description}).
Finally, I assessed the statistical significances of the differences between the mean
scores of the best estimators by paired $t$-tests of the scores on the cross-validation
folds.
I describe these methods in more detail in \cref{sec:subtask-1,sec:subtask-2}.

\import{}{figure-cross-validation.tex}

\section{Data analysis}
\label{sec:data-analysis}

\import{}{table-features.tex}

The data provided for sub-task~1 is recorded at hourly intervals with $n = 54385$
instances across the 75 stations.
A summary of its features and the distributional characteristics of the non-temporal
quantitative features are given in \cref{tab:features,tab:feature-stats}.
This analysis revealed that many of the features, including the target variable
\texttt{bikes}, are missing for $n = 73$ instances, which were henceforth excluded.
Additionally, the `profile' features, i.e., the features derived from the numbers of
available bikes at preceding times, are not defined for the first week of instances at
each station.
Hence, they are missing for approximately $\frac{1}{4}$ of the instances.
The meteorological features are constant for all stations at a given timestamp.
Finally, some features have a natural range -- in particular, the number of available
bikes at a given station is bounded by zero and the number of docks at that station
(\cref{sec:feature-engineering}).

\subsection{Feature selection}
\label{sec:feature-selection}

\import{}{table-feature-stats.tex}

Intuitively, features that have zero variance are not informative for regression
analysis, so I automatically excluded them.\footnote{See
  \sklearn{feature\_selection}{VarianceThreshold}.
}
In the case of sub-task~1, the available data is limited to the month of October 2014,
so these included the month and year.
The `station' features (\cref{tab:features}) are constant for all instances at a given
station, so were likewise excluded in this case.
Finally, the \texttt{precipitation} feature is zero for all instances.
Correlations between features are undesirable in regression analysis
\parencite{Alin2010}, so I also sought to identify and exclude redundant features.
To determine these, I computed the Pearson correlation coefficients between pairs of
quantitative features (\cref{fig:correlations}), which yielded the following:
\begin{itemize}
  \item \texttt{\bikesavgfull} and \texttt{\bikesavgshort} are perfectly correlated ($r = 1.00$).
  \item \texttt{\bikeshdiffavgfull} and \texttt{\bikeshdiffavgshort} are perfectly correlated ($r  = 1.00$).
  \item \texttt{\windspeedmax} and \texttt{\windspeedavg} are highly correlated ($r = 0.96$).
\end{itemize}
Hence, for sub-task~1, the second of each of these pairs of features was also excluded.
For sub-task~2, the `profile' features were retained because they are inputs to the
pre-trained linear models (\cref{tab:2:features}).

\import{}{figure-correlations.tex}

\subsection{Feature engineering}
\label{sec:feature-engineering}

For sub-task~1, the bounds on the number of available bikes at a given station were
enforced by predicting the \emph{fraction} of bikes, i.e., the number of bikes divided
by the number of docks at the station.
I implemented this by extending the \texttt{TransformedTargetRegressor} meta-estimator
to permit data-dependent transforms.\footnote{See
  \sklearn{compose}{TransformedTargetRegressor}.
}
Another possible transformation of the target variable would have been to predict the
\emph{change} in the number of available bikes, but I did not explore this option.

\import{}{figure-weekday.tex}

Initially, I investigated introducing derived temporal features, e.g., by discretizing
the hour of the day into `day' and `night' intervals.
However, as shown in \cref{fig:weekday}, the average fraction of available bikes
throughout the day differs substantially between the training data and the data from
the first ten stations (\cref{sec:task-description}).
Hence, I considered that a feature of this kind would be unlikely to generalize well to
the test data.
Additionally, having elected to investigate tree models (\cref{sec:subtask-1}), which
partition the spaces of quantitative features \parencite[155]{Flach2012},
discretization may be irrelevant.

\section{Sub-task 1}
\label{sec:subtask-1}

\subsection{Model classes}
\label{sec:subtask-1:model-classes}

Gradient-boosted decision trees are a popular choice of model class for time-series
forecasting problems \parencite{Bojer2021}.
In particular, \texttt{scikit-learn} provides an optimized implementation of
gradient-boosted decision trees\footnote{See
  \sklearn{ensemble}{HistGradientBoostingRegressor}.
} inspired by
LightGBM \parencite{Ke2017}.
An advantage of this model class is that it supports missing values, which are evident
in the data (\cref{tab:feature-stats}).
For both cases of sub-task~1, I elected to compare the performance of individual
decision trees and gradient-boosted decision trees.
As a baseline, I predicted the arithmetic mean of the fraction of available bikes.

The best estimator for each model class was determined by grid search with ten-fold
cross-validation (\cref{sec:approach}).
However, with a separate model for each of the 75 stations, I found that it was
computationally expensive to perform grid search over a wide range of hyperparameters.
Hence, I constrained the search space by first searching a wider range for the case of
a single model for all stations, then varying the parameters about the values of the
best estimator for the case of a separate model for each station.
The resultant parameter grids are given in \cref{tab:chart-parameters-subtask-1}.

\begin{table}
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{ll}
      \toprule
      Parameter                   & Values
      \\
      \midrule
      \texttt{criterion}          & \texttt{absolute\_error}
      \\
      \midrule
      \texttt{max\_depth}         & \None, 10, 20, 50
      \\
      \texttt{min\_samples\_leaf} & 5, 10, 20
      \\
      \texttt{max\_leaf\_nodes}   & \None, 7, 15, 31
      \\
      \bottomrule
    \end{tabular}
    \caption{Decision tree}
    \label{tab:chart-parameters-subtask-1-1}
  \end{subfigure}
  \par\bigskip\bigskip
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{ll}
      \toprule
      Parameter                   & Values
      \\
      \midrule
      \texttt{loss}               & \texttt{absolute\_error}
      \\
      \texttt{scoring}            & \texttt{neg\_mean\_absolute\_error}
      \\
      \midrule
      \texttt{l2\_regularization} & 0.1, 0.2, \underline{0.5}
      \\
      \texttt{max\_depth}         & \None, 5, 10, \underline{20}, 50
      \\
      \texttt{max\_iter}          & 10, 20, 50, 100, \underline{200}, 500
      \\
      \texttt{max\_leaf\_nodes}   & \None, \underline{15}, 31, 63
      \\
      \texttt{min\_samples\_leaf} & 1, 2, 5, \underline{10}, 20, 50
      \\
      \bottomrule
    \end{tabular}
    \caption{Gradient-boosted decision tree}
    \label{tab:chart-parameters-subtask-1-2}
  \end{subfigure}
  \caption{The parameter grids over which I performed hyperparameter search for sub-task~1.
    Except where stated, the default values of the parameters were used.
    I fixed the scoring criteria to the mean absolute error, following the task
    description.
    The best parameters for the case of a single model for all stations are underlined.
  }
  \label{tab:chart-parameters-subtask-1}
\end{table}

\subsection{Results}
\label{sec:subtask-1:results}

The mean scores achieved by the best estimator for each model class over the
cross-validation folds and stations are listed in \cref{tab:subtask-1:results}.
With a separate model for each station, there is greater variance in the mean score
because there are 75 times as many cross-validation folds with commensurately fewer
instances per fold.
Intuitively, the baseline achieved a lower score with a separate model for each
station.
For both cases of sub-task~1, both model classes achieved a lower score than the
baseline.

\begin{table}
  \centering
  \begin{subfigure}{0.49\textwidth}
    \centering
    \begin{tabular}{lrr}
      \toprule
      Model                          & $\mu$ & $\sigma^2$
      \\
      \midrule
      Baseline                       & 4.43  & 3.82
      \\
      Decision tree                  & 3.38  & 3.05
      \\
      Gradient-boosted decision tree & 3.23  & 2.54
      \\
      \bottomrule
    \end{tabular}
    \caption{Separate model for each station}
    \label{tab:subtask-1:results-1}
  \end{subfigure}
  % \par\bigskip\bigskip
  \begin{subfigure}{0.49\textwidth}
    \centering
    \begin{tabular}{lrr}
      \toprule
      Model                          & $\mu$ & $\sigma^2$
      \\
      \midrule
      Baseline                       & 5.45  & 0.06
      \\
      Decision tree                  & 3.11  & 0.05
      \\
      Gradient-boosted decision tree & 2.60  & 0.08
      \\
      % \texttt{LGBMRegressor}                 & 2.76  & 0.18
      % \\
      \bottomrule
    \end{tabular}
    \caption{Single model for all stations}
    \label{tab:subtask-1:results-2}
  \end{subfigure}
  \caption{The mean scores achieved by the model classes for each case of sub-task~1.
    In \cref{tab:subtask-1:results-1}, $\mu$ is the mean of the scores for the 75 stations
    and ten cross-validation folds.
    In \cref{tab:subtask-1:results-2}, $\mu$ is the mean of the scores for the ten
    cross-validation folds only.
  }
  \label{tab:subtask-1:results}
\end{table}

To determine whether the differences between the mean scores achieved by the best
estimators were statistically significant, I performed paired $t$-tests of the scores
on the cross-validation folds.
The null hypothesis was that the mean scores of the best estimators were equal, i.e.,
that the differences between them were due to chance.
I did not compare the scores between estimators for the different cases of this
sub-task because the samples are not paired.
With a separate model for each station, both decision trees and gradient-boosted
decision trees performed significantly better than the baseline for roughly
$\frac{2}{3}$ of the stations (\cref{tab:subtask-1:results-t-tests-1}).
However, gradient-boosted decision trees only performed significantly better than
decision trees for eight stations.
With a single model for all stations, the results were more decisive -- the best
gradient-boosted decision tree performed significantly better than the best decision
tree (\cref{tab:subtask-1:results-t-tests-2}).

\begin{table}
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{llrr}
      \toprule
      Model 1       & Model 2                        & $t > 0$ & $p < 0.05$
      \\
      \midrule
      Baseline      & Decision tree                  & 71      & 47
      \\
      Baseline      & Gradient-boosted decision tree & 73      & 49
      \\
      Decision tree & Gradient-boosted decision tree & 47      & 8
      \\
      \bottomrule
    \end{tabular}
    \caption{
      Separate models for each station.
      The numbers of positive $t$-statistics and significant $p$-values of the mean scores of
      the best estimators for each model class on the ten cross-validation folds for each of
      the 75 stations.
    }
    \label{tab:subtask-1:results-t-tests-1}
  \end{subfigure}
  \par\bigskip\bigskip
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{llrr}
      \toprule
      Model 1       & Model 2                        & $t$-statistic & $p$-value
      \\
      \midrule
      Baseline      & Decision tree                  & 25.2          & \num{1.16e-9}
      \\
      Baseline      & Gradient-boosted decision tree & 29.4          & \num{2.99e-10}
      \\
      Decision tree & Gradient-boosted decision tree & 13.2          & \num{3.43e-7}
      \\
      \bottomrule
    \end{tabular}
    \caption{ Single model for all stations.
      The $t$-statistics and $p$-values for paired $t$-tests of the mean scores of the best
      estimators for each model class on the ten cross-validation folds for all 75 stations.
    }
    \label{tab:subtask-1:results-t-tests-2}
  \end{subfigure}
  \caption{The results of paired $t$-tests of the scores achieved by the best
    estimators for each model class for the cases of sub-task~1.
    A positive $t$-statistic indicates that `Model 2' achieved a lower mean score than
    `Model 1'.
  }
\end{table}

\section{Sub-task 2}
\label{sec:subtask-2}

\subsection{Model classes}
\label{sec:subtask-2:model-classes}

The available information for sub-task 2 is a set of linear models that were trained on
the number of available bikes at each of a separate set of 200 stations
(\cref{sec:task-description}).
For each station, there are six models generated by the \texttt{rlm} function from the
R package MASS.\footnote{See
  \href{https://www.rdocumentation.org/packages/MASS/versions/7.3-54/topics/rlm}{\texttt{https://www.rdocumentation.org/packages/MASS/versions/7.3-54/topics/rlm}}.
}
Each of the models uses a different set of features, which are listed in
\cref{tab:2:features}.
Unlike the gradient-boosted decision tree implementation in \texttt{scikit-learn}, the
pre-trained models do not support missing values, so I imputed these\footnote{See
  \sklearn{impute}{SimpleImputer}.
} with the mean value of the feature over the training data.
I used stacked generalization\footnote{See \sklearn{ensemble}{StackingRegressor}.
} to combine the predictions of the models \parencite{Wolpert1992}.
This model class is a meta-estimator that trains a second-level regressor on the
predictions of a set of first-level regressors (the pre-trained models).

\begin{table}
  \centering
  \newcommand{\rlmtablerow}[7]{#1 & #7 & #2 & #3 & #4 & #5 & #6 \\}
  \begin{tabular}{lllllll}

    \rlmtablerow{Model}{\rot{\bikesh{}}}{\rot{\bikesavgfull{}}}{\rot{\bikesavgshort{}}}{\rot{\bikeshdiffavgfull{}}}{\rot{\bikeshdiffavgshort{}}}{\rot{\texttt{temperature}}}

    \midrule

    \rlmtablerow{\rlmfull{}}{\checkmark}{\checkmark}{}{\checkmark}{}{}

    \rlmtablerow{\rlmfulltemp{}}{\checkmark}{\checkmark}{}{\checkmark}{}{\checkmark}

    \rlmtablerow{\rlmshort{}}{\checkmark}{}{\checkmark}{}{\checkmark}{}

    \rlmtablerow{\rlmshortfull{}}{\checkmark}{\checkmark}{\checkmark}{\checkmark}{\checkmark}{}

    \rlmtablerow{\rlmshortfulltemp{}}{\checkmark}{\checkmark}{\checkmark}{\checkmark}{\checkmark}{\checkmark}

    \rlmtablerow{\rlmshorttemp{}}{\checkmark}{}{\checkmark}{}{\checkmark}{\checkmark}

    \bottomrule
  \end{tabular}
  \caption{The features used by the different kinds of linear
    models for sub-task~2.
    The features are listed in \cref{tab:features} and follow the same ordering.
  }
  \label{tab:2:features}
\end{table}

For this sub-task, I also excluded zero-variance and highly correlated features but
retained the `profile' features (\cref{sec:feature-selection}).
However, the \texttt{full} and \texttt{short} features are perfectly correlated, and
the latter were excluded for sub-task~1.
Therefore, I first assessed the performance of the different kinds of pre-trained
models.
A box plot of the mean scores achieved on the data for all 75 stations is shown in
\cref{fig:2:box-plot}.
This shows that individual \rlmshort{} and \rlmshorttemp{} models generally achieved
lower scores.
However, with the default second-level regressor (ridge regression), the results of
stacked generalization with each kind of model were very similar, so I elected to
include all the models in the following analysis.
For the second-level regressor, I opted to compare ridge regression, decision trees,
and gradient-boosted decision trees, as in \cref{sec:subtask-1:model-classes}.
The best second-level regressor was likewise determined by grid search with ten-fold
cross-validation (\cref{sec:approach}).

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        width=5in,
        height=2in,
        cycle list/Paired,
        xlabel={Mean score},
        y dir=reverse,
        ytick pos=right,
        ytick={1,2,3,4,5,6},
        yticklabels={\rlmfull{}, \rlmfulltemp{}, \rlmshort{}, \rlmshortfull{}, \rlmshortfulltemp{}, \rlmshorttemp{}},
      ]
      \addplot+ [boxplot] table [y index=0] {../more_bikes/analysis/rlm/rlm_mean_scores_full.csv};
      \addplot+ [boxplot] table [y index=0] {../more_bikes/analysis/rlm/rlm_mean_scores_full_temp.csv};
      \addplot+ [boxplot] table [y index=0] {../more_bikes/analysis/rlm/rlm_mean_scores_short.csv};
      \addplot+ [boxplot] table [y index=0] {../more_bikes/analysis/rlm/rlm_mean_scores_short_full.csv};
      \addplot+ [boxplot] table [y index=0] {../more_bikes/analysis/rlm/rlm_mean_scores_short_full_temp.csv};
      \addplot+ [boxplot] table [y index=0] {../more_bikes/analysis/rlm/rlm_mean_scores_short_temp.csv};
    \end{axis}
  \end{tikzpicture}
  \caption{A box plot of the mean scores achieved by the different kinds of models for
    sub-task~2 over the ten cross-validation folds of the data from the 75 stations for
    the month of October 2014, i.e., the same data as for the second case of sub-task~1.
    The kinds of models are listed in \cref{tab:2:features}.
  }
  \label{fig:2:box-plot}
\end{figure}

\subsection{Results}
\label{sec:subtask-2:results}

\printbibliography

\end{document}
