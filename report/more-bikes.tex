\documentclass[11pt]{extarticle}
\usepackage{more-bikes}

% Shortcuts
\newcommand{\isholiday}{\texttt{is\_holiday}}
\newcommand{\windspeedmax}{\texttt{wind\_speed\_max}}
\newcommand{\windspeedavg}{\texttt{wind\_speed\_avg}}
\newcommand{\winddirection}{\texttt{wind\_direction}}
\newcommand{\bikesavgfull}{\texttt{bikes\_avg\_full}}
\newcommand{\bikesavgshort}{\texttt{bikes\_avg\_short}}
\newcommand{\bikesh}{\texttt{bikes\_3h}}
\newcommand{\bikeshdiffavgfull}{\texttt{bikes\_3h\_diff\_avg\_full}}
\newcommand{\bikeshdiffavgshort}{\texttt{bikes\_3h\_diff\_avg\_short}}

\newcommand{\rlmfull}{\texttt{rlm\_full}}
\newcommand{\rlmfulltemp}{\texttt{rlm\_full\_temp}}
\newcommand{\rlmshort}{\texttt{rlm\_short}}
\newcommand{\rlmshortfull}{\texttt{rlm\_short\_full}}
\newcommand{\rlmshortfulltemp}{\texttt{rlm\_short\_full\_temp}}
\newcommand{\rlmshorttemp}{\texttt{rlm\_short\_temp}}

\newcommand{\None}{\texttt{None}}

\newcommand{\kaggle}[1]{\href{https://www.kaggle.com/c/morebikes2023/#1}{\lstinline|https://www.kaggle.com/c/morebikes2023/#1|}}
\newcommand{\sklearn}[2]{\href{https://scikit-learn.org/stable/modules/generated/sklearn.#1.#2.html}{\lstinline|sklearn.#1.#2|}}
\newcommand{\skl}[2]{\href{https://scikit-learn.org/stable/modules/generated/sklearn.#1.#2.html}{\lstinline|#2|}}

\newcommand{\subfigurespace}{\par\bigskip}

\begin{document}

\title{`More Bikes': Experiments in Univariate Regression}
\author[]{Tim Lawson}
\affil[]{University of Bristol}
\affil[]{\texttt{tim.lawson@bristol.ac.uk}}
\date{}

\maketitle

\begin{abstract}
  This report describes my approach to the `More Bikes' Kaggle competition, which
  involved predicting the number of available bikes at a set of rental stations in
  Valencia, Spain.
  I investigated the performance of decision trees, gradient-boosted decision trees, and
  stacked generalization with pre-trained linear models.
  Generally, I found that gradient-boosted decision trees performed significantly better
  than decision trees, and both performed significantly better than the baseline of
  predicting the arithmetic mean of the fraction of available bikes.
  Stacked generalization with pre-trained linear models performed competitively with
  gradient-boosted decision trees.
\end{abstract}

\section{Task description}
\label{sec:task-description}

The assignment was organized as a Kaggle competition.\footnote{See \kaggle{overview}.
}
The task is to predict the number of available bikes at 75 rental stations in three
hours' time between November 2014 and January 2015, i.e., a supervised univariate
regression problem.
It is divided into three sub-tasks, which differ in the available information.
For sub-task~1, the data from the 75 stations for the month of October 2014 is provided
(\cref{sec:subtask-1}).
This sub-task may be approached by building a separate model for each station or a
single model for all stations.
For sub-task~2, a set of linear models that were trained on the data from a separate
set of 200 stations is provided (\cref{sec:subtask-2}).
Finally, for sub-task~3, both sources of information may be used.
Additionally, the data from the first ten stations between June 2012 and October 2014
is provided for analysis (\cref{sec:data-analysis}).

The predictions were evaluated by the mean absolute error (MAE) between the predicted
and true numbers of available bikes.
Hereafter, the MAE is referred to as the `score'.
\begin{equation}
  \label{eq:mae}
  \text{MAE} = \frac{1}{n} \sum_{i = 1}^n \lvert y_i - \hat{y}_i \rvert
\end{equation}
The evaluation data was not made available to the competition participants, but the
score achieved on a held-out test set was reported on the task
leaderboard.\footnote{See \kaggle{leaderboard}.
}
I give the score achieved on this test set by the best estimators for each model class
alongside the mean scores on cross-validation folds of the data provided for sub-task~1
in \cref{tab:subtask-1:results,tab:subtask-2:results}.

\section{Approach}
\label{sec:approach}

I used the \texttt{scikit-learn} Python package \parencite{Pedregosa2011} throughout
this report.\footnote{The underlying code is available at
  \github{https://github.com/tslwn/more-bikes}{tslwn/more-bikes}.
}
Preprocessing and feature selection were performed by \emph{estimators} that
implemented the \emph{transformer} interface, prediction was performed by estimators
that implemented the \emph{predictor} interface, and estimators were composed into
\texttt{Pipeline} objects over which hyperparameter search was performed
\parencite[4-9]{Buitinck2013}.

Generally, standard $k$-fold cross-validation is disfavoured for time-series data due
to the inherent correlation between successive folds \parencite{Bergmeir2018}.
Instead, I used nested time-series cross-validation\footnote{See
  \sklearn{model\_selection}{TimeSeriesSplit}.
} with ten folds, which is illustrated in
\cref{fig:cross-validation}.
I determined the best estimator for each model class by grid search\footnote{See
  \sklearn{model\_selection}{GridSearchCV} and
  \skl{model\_selection}{HalvingGridSearchCV}.
} with the mean absolute error as the scoring function, following the task description.
Finally, I assessed the statistical significances of the differences between the mean
scores of the best estimators by paired $t$-tests of the scores on the cross-validation
folds.
I describe these methods in more detail in \cref{sec:subtask-1,sec:subtask-2}.

\import{}{figure-cross-validation.tex}

\section{Data analysis}
\label{sec:data-analysis}

\import{}{table-features.tex}

The data provided for sub-task~1 is recorded at hourly intervals with $n = 54385$
instances across the 75 stations.
A summary of its features and the distributional characteristics of the non-temporal
quantitative features are given in \cref{tab:features,tab:feature-stats}.
This analysis revealed that many of the features and the target variable \texttt{bikes}
are missing for $n = 73$ instances, which were henceforth excluded.
Additionally, the `profile' features, i.e., the features derived from the numbers of
available bikes at preceding times, are not defined for the first week of October 2014
at each station.
Hence, they are missing for approximately $\frac{1}{4}$ of the instances.
The meteorological features are constant for all stations at a given timestamp.
Finally, some features have a natural range -- in particular, the number of available
bikes at a station is bounded by zero and its number of docks
(\cref{sec:feature-engineering}).

\subsection{Feature selection}
\label{sec:feature-selection}

\import{}{table-feature-stats.tex}

Intuitively, features that have zero variance are not informative, so I automatically
excluded them.\footnote{See \sklearn{feature\_selection}{VarianceThreshold}.
}
For sub-task~1, the available data is limited to the month of October 2014, so these
included the month and year.
For the first case of sub-task~1, the `station' features (\cref{tab:features}) were
also included, which are constant for all instances at a given station.
The \texttt{precipitation} feature is zero for all instances.

Correlations between explanatory variables can result in unreliable predictions by a
regression model \parencite{Alin2010}.
\begin{samepage}
  I identified these by computing the Pearson correlation
  coefficients between pairs of quantitative features (\cref{fig:correlations}), which
  yielded the following observations:
  \begin{itemize}
    \item \texttt{\bikesavgfull} and \texttt{\bikesavgshort} are perfectly correlated ($r = 1.00$).
    \item \texttt{\bikeshdiffavgfull} and \texttt{\bikeshdiffavgshort} are perfectly correlated ($r  = 1.00$).
    \item \texttt{\windspeedmax} and \texttt{\windspeedavg} are highly correlated ($r = 0.96$).
  \end{itemize}
\end{samepage}
Hence, for sub-task~1, the second of each of these pairs of features was also excluded.
For sub-task~2, the `profile' features were retained because they are inputs to the
pre-trained linear models (\cref{tab:2:features}).

\import{}{figure-correlations.tex}

\subsection{Feature engineering}
\label{sec:feature-engineering}

For sub-task~1, the bounds on the number of available bikes at each station were
enforced by predicting the \emph{fraction} of bikes, i.e., the number of bikes divided
by the number of docks.
I implemented this by extending the \texttt{TransformedTargetRegressor} meta-estimator
to allow data-dependent transformations of the target variable.\footnote{See
  \sklearn{compose}{TransformedTargetRegressor}.
}
Another potential transformation of the target variable would have been to predict the
\emph{change} in the number of available bikes, but I did not explore this option.

\import{}{figure-weekday.tex}

Initially, I investigated introducing derived temporal features, e.g., by discretizing
the hour of the day into intervals.
However, as shown in \cref{fig:weekday}, the average fraction of available bikes
throughout the day differs substantially between the data from the 75 stations for
October 2014 and the data from the first ten stations between June 2012 and October
2014 (\cref{sec:task-description}).
Hence, I considered that a feature of this kind would be unlikely to generalize well to
the test data.
Additionally, because I chose to investigate tree models (\cref{sec:subtask-1}), which
partition the spaces of quantitative features \parencite[155]{Flach2012},
discretization may be irrelevant.

\section{Sub-task 1}
\label{sec:subtask-1}

\subsection{Model classes}
\label{sec:subtask-1:model-classes}

Gradient-boosted decision trees are a popular choice of model class for time-series
forecasting problems \parencite{Bojer2021}.
In particular, \texttt{scikit-learn} provides an optimized implementation of
gradient-boosted decision trees inspired by LightGBM \parencite{Ke2017}.
An advantage of this model class is that it supports missing values, which are evident
in the data (\cref{tab:feature-stats}).
For both cases of sub-task~1, I elected to compare the performance of individual
decision trees and gradient-boosted decision trees.
As a baseline, I predicted the arithmetic mean of the fraction of available bikes.

The best estimator for each model class was determined by grid search with ten-fold
cross-validation.
With a separate model for each of the 75 stations, I found that it was too
computationally expensive to perform grid search over a wide range of hyperparameters.
Hence, I performed grid search for the case of a single model for all stations, then
chose the values of the best estimator for the case of a separate model for each
station.
I fixed the scoring criteria to the mean absolute error, as per \cref{sec:approach}.
The resultant parameter values are listed in \cref{tab:chart-parameters-subtask-1}.

\begin{table}
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{ll}
      \toprule
      Parameter                   & Values
      \\
      \midrule
      \texttt{criterion}          & \texttt{absolute\_error}
      \\
      \midrule
      \texttt{max\_depth}         & \underline{\None}, 1, 2, 5, 10, 20, 50
      \\
      \texttt{max\_leaf\_nodes}   & \None, \underline{7}, 15, 31, 63
      \\
      \texttt{min\_samples\_leaf} & 1, 2, 5, 10, \underline{20}, 50, 100
      \\
      \bottomrule
    \end{tabular}
    \caption{Decision tree}
    \label{tab:chart-parameters-subtask-1-1}
  \end{subfigure}
  \subfigurespace
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{ll}
      \toprule
      Parameter                   & Values
      \\
      \midrule
      \texttt{loss}               & \texttt{absolute\_error}
      \\
      \texttt{scoring}            & \texttt{neg\_mean\_absolute\_error}
      \\
      \midrule
      \texttt{l2\_regularization} & 0.1, \underline{0.2}, 0.5, 1.0
      \\
      \texttt{learning\_rate}     & 0.02, 0.05, \underline{0.1}, 0.2, 0.5
      \\
      \texttt{max\_depth}         & \None, 1, 2, 5, 10, 20, \underline{50}, 100
      \\
      \texttt{max\_iter}          & 20, 50, \underline{100}, 200, 500
      \\
      \texttt{max\_leaf\_nodes}   & \None, 7, \underline{15}, 31, 63
      \\
      \texttt{min\_samples\_leaf} & 1, \underline{2}, 5, 10, 20, 50, 100
      \\
      \bottomrule
    \end{tabular}
    \caption{Gradient-boosted decision tree}
    \label{tab:chart-parameters-subtask-1-2}
  \end{subfigure}
  \caption{The parameter grids over which I performed hyperparameter search for sub-task~1.
    Except where stated, the default values were used, and the parameters of the best
    estimator are underlined.
    For a description of the parameters and their default values, see
    \sklearn{tree}{DecisionTreeRegressor} and
    \sklearn{ensemble}{HistGradientBoostingRegressor}.
  }
  \label{tab:chart-parameters-subtask-1}
\end{table}

\subsection{Results}
\label{sec:subtask-1:results}

The mean scores achieved by the best estimator for each model class over the
cross-validation folds and stations are listed in \cref{tab:subtask-1:results}.
With a separate model for each station, there is greater variance in the mean score
because there are 75 times as many cross-validation folds with commensurately fewer
instances per fold.
Intuitively, the baseline achieved a lower score with a separate model for each
station.
For both cases of sub-task~1, both model classes achieved a lower score than the
baseline.

\begin{table}
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{lrrr}
      \toprule
      Model                          & $\mu$ & $\sigma^2$ & Test
      \\
      \midrule
      Baseline                       & 4.43  & 3.82       & 4.47
      \\
      Decision tree                  & 3.53  & 3.52       & 3.01
      \\
      Gradient-boosted decision tree & 3.37  & 2.61       & 3.23
      \\
      \bottomrule
    \end{tabular}
    \caption{Separate models for each station. $\mu$ is the mean of the scores for the 75 stations
      and ten cross-validation folds.}
    \label{tab:subtask-1:results-1}
  \end{subfigure}
  \subfigurespace
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{lrrr}
      \toprule
      Model                          & $\mu$ & $\sigma^2$ & Test
      \\
      \midrule
      Baseline                       & 5.45  & 0.06       & 5.38
      \\
      Decision tree                  & 3.51  & 0.08       & TODO
      \\
      Gradient-boosted decision tree & 2.60  & 0.08       & TODO
      \\
      \bottomrule
    \end{tabular}
    \caption{Single model for all stations. $\mu$ is the mean of the scores for the ten
      cross-validation folds only.}
    \label{tab:subtask-1:results-2}
  \end{subfigure}
  \caption{The mean scores and variances achieved by the best estimators for each model
    class on the data provided for sub-task~1, and the corresponding score on the
    held-out test set (\cref{sec:task-description}).
  }
  \label{tab:subtask-1:results}
\end{table}

To determine whether the differences between the mean scores achieved by the best
estimators were statistically significant, I performed paired $t$-tests of the scores
on the cross-validation folds.
The null hypothesis was that the differences between the mean scores were due to
chance.
I did not compare the scores between estimators for the different cases of this
sub-task because the samples are not paired.
With separate models for each station, the best decision trees and gradient-boosted
decision trees performed significantly better than the baseline for more than half of
the stations (\cref{tab:subtask-1:t-tests-1}).
The best gradient-boosted decision trees only performed significantly better than the
best decision trees for seven stations.
With a single model for all stations, the best gradient-boosted decision tree performed
significantly better than the best decision tree (\cref{tab:subtask-1:t-tests-2}).

\begin{table}
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{llrr}
      \toprule
      Model 1       & Model 2                        & $t > 0$ & $p < 0.05$
      \\
      \midrule
      Baseline      & Decision tree                  & 70      & 39
      \\
      Baseline      & Gradient-boosted decision tree & 70      & 44
      \\
      Decision tree & Gradient-boosted decision tree & 44      & 7
      \\
      \bottomrule
    \end{tabular}
    \caption{Separate models for each station.
      The numbers of positive $t$-statistics and significant $p$-values of the mean scores of
      the best estimators for each model class on the ten cross-validation folds for each of
      the 75 stations.
    }
    \label{tab:subtask-1:t-tests-1}
  \end{subfigure}
  \subfigurespace
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{llrr}
      \toprule
      Model 1       & Model 2                        & $t$-statistic & $p$-value
      \\
      \midrule
      Baseline      & Decision tree                  & 18.6          & \num{1.70e-9}
      \\
      Baseline      & Gradient-boosted decision tree & 31.6          & \num{1.55e-10}
      \\
      Decision tree & Gradient-boosted decision tree & 8.2           & \num{1.83e-5}
      \\
      \bottomrule
    \end{tabular}
    \caption{Single model for all stations.
      The $t$-statistics and $p$-values for paired $t$-tests of the mean scores of the best
      estimators for each model class on the ten cross-validation folds for all 75 stations.
    }
    \label{tab:subtask-1:t-tests-2}
  \end{subfigure}
  \caption{The results of paired $t$-tests of the scores achieved by the best
    estimators for each model class.
    A positive $t$-statistic indicates that `Model 2' achieved a lower mean score than
    `Model 1'.
  }
  \label{tab:subtask-1:t-tests}
\end{table}

\section{Sub-task 2}
\label{sec:subtask-2}

\subsection{Model classes}
\label{sec:subtask-2:model-classes}

The information provided for sub-task~2 is a set of linear models that were trained on
the data from a separate set of 200 stations.
There are six models for each station, each of which uses a different set of features,
listed in \cref{tab:2:features}.
Unlike the gradient-boosted decision tree implementation in \texttt{scikit-learn},
these models do not support missing values, so I imputed them with the mean value of
the feature over the data provided for sub-task~1.
I used \sklearn{ensemble}{StackingRegressor} to combine the predictions of the models
by stacked generalization \parencite{Wolpert1992}.
This model class is a meta-estimator that trains a second-level regressor on the
predictions of a set of first-level regressors -- in this case, the pre-trained models.

\begin{table}
  \centering
  \newcommand{\rlmtablerow}[7]{#1 & #7 & #2 & #3 & #4 & #5 & #6 \\}
  \begin{tabular}{lllllll}

    \rlmtablerow{Model}{\rot{\bikesh{}}}{\rot{\bikesavgfull{}}}{\rot{\bikesavgshort{}}}{\rot{\bikeshdiffavgfull{}}}{\rot{\bikeshdiffavgshort{}}}{\rot{\texttt{temperature}}}

    \midrule

    \rlmtablerow{\rlmfull{}}{\checkmark}{\checkmark}{}{\checkmark}{}{}

    \rlmtablerow{\rlmfulltemp{}}{\checkmark}{\checkmark}{}{\checkmark}{}{\checkmark}

    \rlmtablerow{\rlmshort{}}{\checkmark}{}{\checkmark}{}{\checkmark}{}

    \rlmtablerow{\rlmshortfull{}}{\checkmark}{\checkmark}{\checkmark}{\checkmark}{\checkmark}{}

    \rlmtablerow{\rlmshortfulltemp{}}{\checkmark}{\checkmark}{\checkmark}{\checkmark}{\checkmark}{\checkmark}

    \rlmtablerow{\rlmshorttemp{}}{\checkmark}{}{\checkmark}{}{\checkmark}{\checkmark}

    \bottomrule
  \end{tabular}
  \caption{The features used by the different kinds of linear
    models for sub-task~2.
    The features are listed in \cref{tab:features} and follow the same ordering.
  }
  \label{tab:2:features}
\end{table}

For this sub-task, I also excluded zero-variance and highly correlated features, but
retained the `profile' features (\cref{sec:feature-selection}).
However, the \texttt{full} and \texttt{short} features are perfectly correlated, and
the latter were excluded for sub-task~1.
Therefore, I first assessed the performance of the different kinds of pre-trained
models.
A box plot of the mean scores achieved on the data provided for sub-task~1 is shown in
\cref{fig:2:box-plot}.
It indicates that individual \rlmshort{} and \rlmshorttemp{} models generally achieved
lower mean scores.
However, with the default second-level regressor (ridge regression), the results of
stacked generalization with each kind of model were very similar, so I elected to
include all the models in the final analysis.
For the second-level regressor, I compared ridge regression, decision trees, and
gradient-boosted decision trees, as in \cref{sec:subtask-1:model-classes}.
The best second-level regressor was likewise determined by grid search with ten-fold
cross-validation (\cref{sec:approach}).

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        width=5in,
        height=2in,
        cycle list/Paired,
        xlabel={Mean score},
        y dir=reverse,
        ytick pos=right,
        ytick={1,2,3,4,5,6},
        yticklabels={\rlmfull{}, \rlmfulltemp{}, \rlmshort{}, \rlmshortfull{}, \rlmshortfulltemp{}, \rlmshorttemp{}},
      ]
      \addplot+ [boxplot] table [y index=0] {../more_bikes/analysis/rlm/rlm_mean_scores_full.csv};
      \addplot+ [boxplot] table [y index=0] {../more_bikes/analysis/rlm/rlm_mean_scores_full_temp.csv};
      \addplot+ [boxplot] table [y index=0] {../more_bikes/analysis/rlm/rlm_mean_scores_short.csv};
      \addplot+ [boxplot] table [y index=0] {../more_bikes/analysis/rlm/rlm_mean_scores_short_full.csv};
      \addplot+ [boxplot] table [y index=0] {../more_bikes/analysis/rlm/rlm_mean_scores_short_full_temp.csv};
      \addplot+ [boxplot] table [y index=0] {../more_bikes/analysis/rlm/rlm_mean_scores_short_temp.csv};
    \end{axis}
  \end{tikzpicture}
  \caption{A box plot of the mean scores achieved by the different kinds of models for
    sub-task~2 over the ten cross-validation folds of the data from the 75 stations for
    the month of October 2014, i.e., the same data as for the second case of sub-task~1.
    The kinds of models are listed in \cref{tab:2:features}.
  }
  \label{fig:2:box-plot}
\end{figure}

\subsection{Results}
\label{sec:subtask-2:results}

The mean scores achieved by the best estimator for each model class over the
cross-validation folds are listed in \cref{tab:subtask-2:results}.
All the model classes achieved a lower score than the baselines in sub-task~1.
As in \cref{sec:subtask-1}, I performed paired $t$-tests of the scores on the
cross-validation folds to determine whether the differences between the mean scores
achieved by the best estimators were statistically significant.
The results are listed in \cref{tab:subtask-2:t-tests}.
Stacked generalization with both ridge regression and gradient-boosted decision trees
as the second-level regressor achieved a lower score than with decision trees, but they
did not perform significantly better than each other.

\begin{table}
  \centering
  \begin{tabular}{lrrr}
    \toprule
    Model                          & $\mu$ & $\sigma^2$ & Test
    \\
    \midrule
    Ridge                          & 2.85  & 0.13       & TODO
    \\
    Decision tree                  & 3.79  & 0.10       & TODO
    \\
    Gradient-boosted decision tree & 2.84  & 0.19       & TODO
    \\
    \bottomrule
  \end{tabular}
  \caption{The mean scores and variances achieved by the best estimators for each model
    class for sub-task~2 on the training data, and the corresponding score
    on the held-out test set (\cref{sec:task-description}).
  }
  \label{tab:subtask-2:results}
\end{table}

\begin{table}
  \centering
  \begin{tabular}{llrr}
    \toprule
    Model 1       & Model 2                        & $t$-statistic & $p$-value
    \\
    \midrule
    Ridge         & Decision tree                  & -20.7         & \num{6.70e-9}
    \\
    Ridge         & Gradient-boosted decision tree & 0.46          & \num{6.55e-1}
    \\
    Decision tree & Gradient-boosted decision tree & 17.2          & \num{3.46e-8}
    \\
    \bottomrule
  \end{tabular}
  \caption{The results of paired $t$-tests of the scores achieved by the best
    estimators for each model class for sub-task~2.
    As in \cref{tab:subtask-1:t-tests}, a positive $t$-statistic indicates that `Model 2'
    achieved a lower mean score than `Model 1'.
  }
  \label{tab:subtask-2:t-tests}
\end{table}

\printbibliography

\end{document}
