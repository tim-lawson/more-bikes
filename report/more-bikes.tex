\documentclass[11pt]{extarticle}
\usepackage{more-bikes}

% Shortcuts
\newcommand{\isholiday}{\texttt{is\_holiday}}
\newcommand{\windspeedmax}{\texttt{wind\_speed\_max}}
\newcommand{\windspeedavg}{\texttt{wind\_speed\_avg}}
\newcommand{\winddirection}{\texttt{wind\_direction}}
\newcommand{\bikesavgfull}{\texttt{bikes\_avg\_full}}
\newcommand{\bikesavgshort}{\texttt{bikes\_avg\_short}}
\newcommand{\bikesh}{\texttt{bikes\_3h}}
\newcommand{\bikeshdiffavgfull}{\texttt{bikes\_3h\_diff\_avg\_full}}
\newcommand{\bikeshdiffavgshort}{\texttt{bikes\_3h\_diff\_avg\_short}}

\newcommand{\rlmfull}{\texttt{rlm\_full}}
\newcommand{\rlmfulltemp}{\texttt{rlm\_full\_temp}}
\newcommand{\rlmshort}{\texttt{rlm\_short}}
\newcommand{\rlmshortfull}{\texttt{rlm\_short\_full}}
\newcommand{\rlmshortfulltemp}{\texttt{rlm\_short\_full\_temp}}
\newcommand{\rlmshorttemp}{\texttt{rlm\_short\_temp}}

\newcommand{\None}{\texttt{None}}

\newcommand{\kaggle}{https://www.kaggle.com/c/morebikes2023/leaderboard}
\newcommand{\sklearn}[2]{\href{https://scikit-learn.org/stable/modules/generated/sklearn.#1.#2.html}{\lstinline|sklearn.#1.#2|}}

\begin{document}

\title{`More Bikes': Experiments in Univariate Regression}
\author[]{Tim Lawson}
\affil[]{University of Bristol}
\affil[]{\texttt{tim.lawson@bristol.ac.uk}}
\date{}

\maketitle

\section{Task description}

The assignment is to predict the number of available bikes at 75 rental stations in
three hours' time for a period of three months from November 2014, i.e., a supervised
univariate regression problem.
It is divided into three sub-tasks, which differ in the information that is available.
For sub-task~1, the number of available bikes at each of the 75 stations for the month
of October 2014 is provided (\cref{sec:results-subtask-1}).
This sub-task may be approached by building a separate model for each station or a
single model for all 75 stations.
For sub-task~2, a set of linear models that were trained on the number of available
bikes at each of a separate set of 200 stations for a year are provided
(\cref{sec:results-subtask-2}).
Finally, for sub-task~3, both sources of information may be used.
Additionally, the number of available bikes at each of ten stations between June 2012
and October 2014 is provided for analysis (\cref{fig:chart-weekday}).

The evaluation data was not available to participants, but the score achieved on a
held-out test set is reported on the task
leaderboard.\footnote{\href{\kaggle}{\texttt{\kaggle}}} The predictions are evaluated
by the mean absolute error (MAE) between the predicted and true numbers of available
bikes over the period of three months, beginning in November 2014.
Hereafter, the MAE is referred to as the `score'.
\begin{equation}
  \label{eq:mae}
  \text{MAE} = \frac{1}{n} \sum_{i = 1}^n \lvert y_i - \hat{y}_i \rvert
\end{equation}

This report begins with a description of the general approach taken to the assignment
in \cref{sec:approach}.
\Cref{sec:data-analysis} presents a preliminary analysis of the data.
Finally, \cref{sec:results-subtask-1,sec:results-subtask-2} detail the results of the
methods applied to each of the sub-tasks.\footnote{The code that produced these results
  is available at \github{https://github.com/tslwn/more-bikes}{tslwn/more-bikes}.
}

\section{Approach}
\label{sec:approach}

The experiments described in this report were performed with the \texttt{scikit-learn}
Python package \parencite{Pedregosa2011}.
In each case, preprocessing and feature selection were performed by \emph{estimators}
that implemented the \emph{transformer} interface, prediction was performed by
estimators that implemented the \emph{predictor} interface, and estimators were
composed into \texttt{Pipeline} objects over which hyperparameter search was performed
\parencite[4-9]{Buitinck2013}.

Generally, standard $k$-fold cross-validation is disfavoured for time-series data due
to the inherent correlation between successive folds \parencite{Bergmeir2018}.
Instead, nested time-series cross-validation\footnote{See
  \sklearn{model\_selection}{TimeSeriesSplit}.
} with ten folds
was used to evaluate the models, which is illustrated in \cref{fig:chart-cross-validation}.
The best estimator for each model class was determined by grid search\footnote{See
  \sklearn{model\_selection}{GridSearchCV} and
  \sklearn{model\_selection}{HalvingGridSearchCV}.
} with the mean absolute
error as the scoring function, following the task description.
Finally, the statistical significances of the differences between the mean scores of
the best estimators were assessed by paired $t$-tests of the scores on the ten
cross-validation folds.
These methods are described in more detail in
\cref{sec:results-subtask-1,sec:results-subtask-2}.

\import{}{figure-cross-validation.tex}

\section{Data analysis}
\label{sec:data-analysis}

\import{}{table-features.tex}

The data is at hourly intervals with $n = 54385$ instances across 75 stations.
A summary of its features and the distributional characteristics of the non-temporal
quantitative features are given in \cref{tab:features,tab:feature-stats}.
This analysis revealed that many of the features, including the target variable
\texttt{bikes}, are missing for $n = 73$ instances, which were excluded in each of the
pipelines.
Additionally, the `profile' features, i.e., the features derived from the numbers of
available bikes at preceding times, are not defined for the first week of instances at
each station.
Hence, they are missing for approximately $\frac{1}{4}$ of the instances.
The meteorological features are constant for all stations at a given timestamp. Some
features have a natural range -- in particular, the number of available bikes at a
given station is bounded by zero and the number of docks at that station
(\cref{sec:feature-engineering}).

\subsection{Feature selection}
\label{sec:feature-selection}

\import{}{table-feature-stats.tex}

Intuitively, features that have zero variance are not informative for regression
analysis and were automatically excluded.\footnote{See
  \sklearn{feature\_selection}{VarianceThreshold}.
}
In the case of sub-task~1, the available data is limited to the month of October 2014;
therefore, these included the month and year.
The `station' features (\cref{tab:features}) are constant for all instances at a given
station, so were likewise excluded in this case.
Finally, the \texttt{precipitation} feature is zero for all instances.

Correlations between features are undesirable in regression analysis
\parencite{Alin2010}.
Therefore, I sought to identify and exclude redundant features.
To determine these, I computed the Pearson correlation coefficients between pairs of
quantitative features (\cref{fig:chart-correlations}), which yielded the following:
\begin{itemize}
  \item \texttt{\bikesavgfull} and \texttt{\bikesavgshort} are fully correlated ($r = 1.00$);
  \item \texttt{\bikeshdiffavgfull} and \texttt{\bikeshdiffavgshort} are fully correlated ($r  = 1.00$);
  \item \texttt{\windspeedmax} and \texttt{\windspeedavg} are highly correlated ($r = 0.96$).
\end{itemize}
Hence, the second of each of these pairs of features was also excluded for sub-task~1.
For sub-task~2, the `profile' features were retained because they are inputs to the
pre-trained linear models (\cref{tab:features-subtask-2}).

\import{}{figure-correlations.tex}

\subsection{Feature engineering}
\label{sec:feature-engineering}

The bounds on the number of available bikes at a given station were enforced for
sub-task~1 by predicting the \emph{fraction} of bikes, i.e., the number of bikes
divided by the number of docks at the station.
I implemented this by extending the \texttt{TransformedTargetRegressor} meta-estimator
to permit data-dependent transforms.\footnote{See
  \sklearn{compose}{TransformedTargetRegressor}.
}
Another possible transformation of the target variable would have been to predict the
\emph{change} in the number of available bikes, but I did not explore this option.

\import{}{figure-weekday.tex}

Initially, I investigated introducing derived temporal features, e.g., by discretizing
the hour of the day into `day' and `night' intervals.
However, as shown in \cref{fig:chart-weekday}, the average fraction of available bikes
at each hour of the day differs substantially between the training data and the data
from the first ten stations.
Hence, I considered that a feature of this kind would be unlikely to generalize well to
the test data.
Additionally, having elected to investigate tree models for sub-task 1
(\cref{sec:results-subtask-1}), which partition the spaces of quantitative features
\parencite[155]{Flach2012}, discretization may be unnecessary.

\section{Sub-task 1: Decision-tree regressors}
\label{sec:results-subtask-1}

\subsection{Model classes}

Gradient-boosted decision trees are a popular choice of model class for time-series
forecasting problems \parencite{Bojer2021}.
In particular, \texttt{scikit-learn} provides an optimized implementation inspired by
LightGBM \parencite{Ke2017}.\footnote{See
  \sklearn{ensemble}{HistGradientBoostingRegressor}.
}
An advantage of this model class is that it supports missing values, which are evident
in the data (\cref{tab:feature-stats}).
For both cases of sub-task 1, I chose to investigate the performance of decision trees
and ensemble methods based on decision trees.
Additionally, I predicted the arithmetic mean of the fraction of available bikes as a
baseline.

The best estimator for each model class was determined by grid search with ten-fold
cross-validation (\cref{sec:approach}).
However, with a separate model for each of 75 stations, I found that it was
computationally expensive to perform grid search over a wide range of hyperparameters.
Hence, I constrained the search space for this case by first searching a wider range
for the case of a single model for all stations, then varying the parameters about the
values of the best estimator.
The resultant parameter grids for decision trees and gradient-boosted decision trees
are given in \cref{tab:chart-parameters-subtask-1}.

\begin{table}
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{ll}
      \toprule
      Parameter                   & Values
      \\
      \midrule
      \texttt{criterion}          & \texttt{absolute\_error}
      \\
      \midrule
      \texttt{max\_depth}         & \None, 10, 20, 50
      \\
      \texttt{min\_samples\_leaf} & 5, 10, 20
      \\
      \texttt{max\_leaf\_nodes}   & \None, 7, 15, 31
      \\
      \bottomrule
    \end{tabular}
    \caption{Decision tree}
    \label{tab:chart-parameters-subtask-1-1}
  \end{subfigure}
  \par\bigskip\bigskip
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{ll}
      \toprule
      Parameter                   & Values
      \\
      \midrule
      \texttt{loss}               & \texttt{absolute\_error}
      \\
      \texttt{scoring}            & \texttt{neg\_mean\_absolute\_error}
      \\
      \midrule
      \texttt{l2\_regularization} & 0.1, 0.2, \underline{0.5}
      \\
      \texttt{max\_depth}         & \None, 5, 10, \underline{20}, 50
      \\
      \texttt{max\_iter}          & 10, 20, 50, 100, \underline{200}, 500
      \\
      \texttt{max\_leaf\_nodes}   & \None, \underline{15}, 31, 63
      \\
      \texttt{min\_samples\_leaf} & 1, 2, 5, \underline{10}, 20, 50
      \\
      \bottomrule
    \end{tabular}
    \caption{Gradient-boosted decision tree}
    \label{tab:chart-parameters-subtask-1-2}
  \end{subfigure}
  \caption{The parameter grids over which hyperparameter search was performed for sub-task~1.
    Except where stated, the default values of the parameters were used.
    Scoring criteria were fixed to the mean absolute error, following the task description.
    The best parameters for the case of a single model for all stations are underlined.
  }
  \label{tab:chart-parameters-subtask-1}
\end{table}

\subsection{Results}

The mean scores achieved by the best estimator for each model class over the
cross-validation folds and stations are listed in \cref{tab:chart-subtask-1}.
With a separate model for each station, there is greater variance in the mean score
because there were fewer instances per cross-validation fold.
Intuitively, the baseline achieved a lower score with a separate model for each
station.
In both cases of sub-task~1, all model classes achieved a lower score than the
baseline.

\begin{table}
  \centering
  \begin{subfigure}{0.49\textwidth}
    \centering
    \begin{tabular}{lrr}
      \toprule
      Model                          & $\mu$ & $\sigma^2$
      \\
      \midrule
      Baseline                       & 4.43  & 3.82
      \\
      Decision tree                  & 3.38  & 3.05
      \\
      Gradient-boosted decision tree & 3.23  & 2.54
      \\
      \bottomrule
    \end{tabular}
    \caption{Separate model for each station}
    \label{tab:chart-subtask-1-1}
  \end{subfigure}
  % \par\bigskip\bigskip
  \begin{subfigure}{0.49\textwidth}
    \centering
    \begin{tabular}{lrr}
      \toprule
      Model                          & $\mu$ & $\sigma^2$
      \\
      \midrule
      Baseline                       & 5.45  & 0.06
      \\
      Decision tree                  & 3.11  & 0.05
      \\
      Gradient-boosted decision tree & 2.60  & 0.08
      \\
      % \texttt{LGBMRegressor}                 & 2.76  & 0.18
      % \\
      \bottomrule
    \end{tabular}
    \caption{Single model for all stations}
    \label{tab:chart-subtask-1-2}
  \end{subfigure}
  \caption{The mean scores achieved by the model classes for each case of sub-task~1.
    In \cref{tab:chart-subtask-1-1}, $\mu$ is the mean of the scores for the 75 stations
    and ten cross-validation folds.
    In \cref{tab:chart-subtask-1-2}, $\mu$ is the mean of the scores for the ten
    cross-validation folds only.
  }
  \label{tab:chart-subtask-1}
\end{table}

To determine whether the differences between the mean scores achieved by the best
estimators for each model class were statistically significant, I performed paired
$t$-tests of the scores on the ten cross-validation folds.
I did not compare the scores between estimators for the different cases of the sub-task
because the samples are different, i.e., not paired.
With a separate model for each station, both individual decision trees and
gradient-boosted decision trees performed significantly better than the baseline for
roughly $\frac{2}{3}$ of the stations (\cref{tab:chart-subtask-1-t-tests-1}).
However, gradient-boosted decision trees performed significantly better than individual
decision trees for $8$ stations only.
With a single model for all stations, the results were more decisive -- the best
gradient-boosted decision tree performed significantly better than the best individual
decision tree (\cref{tab:chart-subtask-1-t-tests-2}).

\begin{table}
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{llrrr}
      \toprule
      Model 1       & Model 2                        & $\mu_t$ & $t > 0$ & $p < 0.05$
      \\
      \midrule
      Baseline      & Decision tree                  & 2.957   & 71      & 47
      \\
      Baseline      & Gradient-boosted decision tree & 3.399   & 73      & 49
      \\
      Decision tree & Gradient-boosted decision tree & 0.452   & 47      & 8
      \\
      \bottomrule
    \end{tabular}
    \caption{
      Separate models for each station.
      The average $t$-statistic ($\mu_t$), number of positive $t$-statistics ($t > 0$), and
      number of significant $p$-values ($p < 0.05$) for paired $t$-tests of the scores
      achieved by the best estimators for each model class on the ten cross-validation folds.
    }
    \label{tab:chart-subtask-1-t-tests-1}
  \end{subfigure}
  \par\bigskip\bigskip
  \begin{subfigure}{\textwidth}
    \centering
    \begin{tabular}{llrr}
      \toprule
      Model 1       & Model 2                        & $t$-statistic & $p$-value
      \\
      \midrule
      Baseline      & Decision tree                  & 25.2          & \num{1.16e-9}
      \\
      Baseline      & Gradient-boosted decision tree & 29.4          & \num{2.99e-10}
      \\
      % Baseline      & LightGBM                       & 23.8 & \num{1.97e-9}
      % \\
      % Decision tree & LightGBM                       & 4.0  & \num{3.23e-3}
      % \\
      % LightGBM      & Gradient-boosted decision tree & 3.1  & \num{1.30e-2}
      % \\
      Decision tree & Gradient-boosted decision tree & 13.2          & \num{3.43e-7}
      \\
      \bottomrule
    \end{tabular}
    \caption{ Single model for all stations.
      The $t$-statistic and $p$-value for paired $t$-tests of the scores achieved by the best
      estimators for each model class on the ten cross-validation folds.
    }
    \label{tab:chart-subtask-1-t-tests-2}
  \end{subfigure}
  \caption{The results of paired $t$-tests of the scores achieved by the best
    estimators for each model class for the cases of sub-task~1.
    A positive $t$-statistic indicates that `Model 2' achieved a lower mean score than
    `Model 1'.
  }
\end{table}

\section{Sub-task 2: Ensembles}
\label{sec:results-subtask-2}

The available information for sub-task 2 is a set of linear models that were trained on
the data for a separate set of 200 stations for a year.
For each station, there are six models that were generated by the \texttt{rlm} function
from the R package MASS.\footnote{See
  \href{https://www.rdocumentation.org/packages/MASS/versions/7.3-54/topics/rlm}{\texttt{https://www.rdocumentation.org/packages/MASS/versions/7.3-54/topics/rlm}}.
}
Each of the models uses a different set of features, which are listed in
\cref{tab:features-subtask-2}.
As discussed in \cref{sec:feature-selection}, the `full' and `short' features are fully
correlated.

\begin{table}
  \centering
  \begin{tabular}{lllllll}
    Model                      & \rot{\bikesh{}}             & \rot{\bikesavgfull{}}      & \rot{\bikesavgshort{}} &
    \rot{\bikeshdiffavgfull{}} & \rot{\bikeshdiffavgshort{}} & \rot{\texttt{temperature}}
    \\
    \midrule \rlmfull          & \checkmark                  & \checkmark                 &                        & \checkmark &            &
    \\
    \rlmfulltemp               &
    \checkmark                 & \checkmark                  &                            & \checkmark             &            & \checkmark
    \\
    \rlmshort                  & \checkmark                  &                            &
    \checkmark                 &                             & \checkmark                 &
    \\
    \rlmshortfull              & \checkmark                  & \checkmark                 & \checkmark             &
    \checkmark                 & \checkmark                  &
    \\
    \rlmshortfulltemp          & \checkmark                  & \checkmark                 & \checkmark             &
    \checkmark                 & \checkmark                  & \checkmark
    \\
    \rlmshorttemp              & \checkmark                  &                            & \checkmark             &            &
    \checkmark                 & \checkmark
    \\
    \bottomrule
  \end{tabular}
  \caption{The features used by the
    linear models for sub-task~2.
    The features are defined in \cref{tab:features}.
  }
  \label{tab:features-subtask-2}
\end{table}

\printbibliography

\end{document}
