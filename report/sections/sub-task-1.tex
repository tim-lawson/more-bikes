\section{Sub-task 1}
\label{sec:sub-task-1}

\subsection{Model classes}
\label{sec:sub-task-1:model-classes}

Gradient-boosted decision trees are a popular choice of model class for time-series
forecasting problems \parencite{Bojer2021}.
In particular, \texttt{scikit-learn} provides an optimized implementation of
gradient-boosted decision trees inspired by LightGBM \parencite{Ke2017}.
An advantage of this model class is that it supports missing values, which are evident
in the data (\cref{tab-features-na}).
For both cases of sub-task~1, I elected to compare the performance of individual
decision trees and gradient-boosted decision trees.
As a baseline, I predicted the arithmetic mean of the fraction of available bikes.

The best estimator for each model class was determined by grid search with ten-fold
cross-validation.
I fixed scoring criteria to the mean absolute error, as per \cref{sec:approach}.
With a separate model for each of the 75 stations, I found that it was too
computationally expensive to perform grid search over a wide range of hyperparameters.
Hence, I performed grid search for the case of (b) a single model for all stations,
then chose the values of the best estimator for the case of (a) a separate model for
each station.
I note that this approach biased the results in favour of case (b), because I did not
optimize the hyperparameters for each station individually.
The parameter values are listed in \cref{sub-task-1:tab-params}.

\begin{table}
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \import{sub-task-1}{tab-params-decision-tree}
    \caption{Decision tree}
    \label{sub-task-1:tab-params-decision-tree}
  \end{subfigure}
  \subfigurespace
  \begin{subfigure}{\textwidth}
    \centering
    \import{sub-task-1}{tab-params-gbdt}
    \caption{Gradient-boosted decision tree}
    \label{sub-task-1:tab-params-gbdt}
  \end{subfigure}
  \caption{
    The parameter values over which I performed grid search.
    Except where stated, the default values were used, and the parameters of the best
    estimator are underlined.
    For a description of the parameters and their default values, see
    \sklearn{tree}{DecisionTreeRegressor} and
    \sklearn{ensemble}{HistGradientBoostingRegressor}.
  }
  \label{sub-task-1:tab-params}
\end{table}

\subsection{Results}
\label{sec:sub-task-1:results}

The mean scores achieved by the best estimators for each model class on the
cross-validation folds and stations are listed in \cref{sub-task-1:tab-results}.
With a separate model for each station, there is greater variance in the mean score
because there are 75 times as many cross-validation folds with commensurately fewer
instances per fold.
Intuitively, the baseline achieved a lower score with a separate model for each
station, i.e., the average fraction of available bikes varies between stations.
For both cases of sub-task~1, decision trees and gradient-boosted decision trees
achieved a lower score than the baseline.

\begin{table}
  \centering
  \begin{subfigure}{0.6\textwidth}
    \centering
    \import{sub-task-1}{tab-results-a}
    \caption{
      Separate models for each station.
      The mean and variance are over the 75 stations and ten cross-validation folds.
    }
    \label{sub-task-1:tab-results-a}
  \end{subfigure}
  \subfigurespace
  \begin{subfigure}{0.6\textwidth}
    \centering
    \import{sub-task-1}{tab-results-b}
    \caption{
      Single model for all stations.
      The mean and variance are over the ten cross-validation folds only.
    }
    \label{sub-task-1:tab-results-b}
  \end{subfigure}
  \caption{ The mean scores and variances of the best estimators for each
    model class on the data provided for sub-task~1, and the corresponding score on the
    held-out test set (\cref{sec:task-description}).
    The best scores on each dataset are underlined.
  }
  \label{sub-task-1:tab-results}
\end{table}

I assessed whether the differences between the mean scores achieved by the best
estimators were statistically significant by performing paired $t$-tests and the
Nemenyi test on the scores on the cross-validation folds.
The null hypothesis was that the differences between the mean scores were due to
chance.
I did not compare the scores between estimators for the different cases of this
sub-task because the samples are different \parencite[354]{Flach2012}.
With separate models for each station, I only performed paired $t$-tests on the scores,
so the $p$-values are not corrected for multiple comparisons
(\cref{sub-task-1:tab-t-tests-a}).
Regardless, this indicated that the best decision trees and gradient-boosted decision
trees performed better than the baseline for more than half of the stations.
The best gradient-boosted decision trees only performed better than the best decision
trees for seven stations.
With a single model for all stations, I also performed the Nemenyi test to correct for
multiple comparisons (\cref{sub-task-1:tab-t-tests-b}).
This indicated that only the best gradient-boosted decision tree performed
significantly better than the baseline, but it did not perform significantly better
than the best decision tree.

\import{sub-task-1}{tab-t-tests}

\begin{table}
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \import{sub-task-1}{tab-t-tests-a}
    \caption{
      Separate models for each station.
      The numbers of positive $t$-statistics and `significant' $p$-values from paired
      $t$-tests on the mean scores of the best estimators for each model class on the ten
      cross-validation folds for each of the 75 stations.
      The $p$-values are not corrected for multiple comparisons, unlike in
      \cref{sub-task-1:tab-t-tests-b}.
    }
    \label{sub-task-1:tab-t-tests-a}
  \end{subfigure}
  \subfigurespace
  \begin{subfigure}{\textwidth}
    \centering
    \import{sub-task-1}{tab-t-tests-b}
    \caption{
      Single model for all stations.
      The $t$-statistics from paired $t$-tests, and $p$-values from the Nemenyi test, on the
      mean scores of the best estimators for each model class on the ten cross-validation
      folds for all 75 stations.
    }
    \label{sub-task-1:tab-t-tests-b}
  \end{subfigure}
  \caption{
    The results of statistical tests on the scores achieved by the best estimators for each model class.
    A positive $t$-statistic indicates that `Model B' achieved a lower mean score than
    `Model A'.
  }
  \label{sub-task-1:tab-t-tests}
\end{table}
